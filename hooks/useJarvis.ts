'use client';

import { useRef, useState, useCallback, useEffect } from 'react';
import { GoogleGenAI, Modality } from '@google/genai';
import { base64ToFloat32, floatToBase64PCM } from '@/lib/audioConverter';
import instructions from '@/lib/instructions.json';
import { say_hello } from '@/app/action/say_hello';

const SAMPLE_RATE = 24000;

// Tool handlers - à¹€à¸žà¸´à¹ˆà¸¡ function à¹ƒà¸«à¸¡à¹ˆà¹„à¸”à¹‰à¸—à¸µà¹ˆà¸™à¸µà¹ˆ
const toolHandlers: Record<string, () => Promise<any>> = {
  say_hello: say_hello,
};

export type JarvisStatus = 'idle' | 'listening' | 'thinking' | 'speaking';

export function useJarvis() {
  const [active, setActive] = useState(false);
  const [status, setStatus] = useState<JarvisStatus>('idle');
  const refs = useRef<any>({});
  const speakTimeout = useRef<NodeJS.Timeout | null>(null);

  // Auto-close if listening for too long
  useEffect(() => {
    let timeout: NodeJS.Timeout;
    if (status === 'listening' && active) {
        timeout = setTimeout(() => {
            console.log('ðŸ’¤ Auto closing due to inactivity...');
            // à¹€à¸£à¸µà¸¢à¸ toggle à¹€à¸žà¸·à¹ˆà¸­à¸›à¸´à¸” (à¸•à¹‰à¸­à¸‡à¹à¸™à¹ˆà¹ƒà¸ˆà¸§à¹ˆà¸²à¸¡à¸±à¸™à¸ˆà¸°à¸›à¸´à¸”)
            // à¹€à¸™à¸·à¹ˆà¸­à¸‡à¸ˆà¸²à¸ toggle à¹ƒà¸Šà¹‰ active state à¹ƒà¸™ closure, à¹€à¸£à¸²à¸­à¸²à¸ˆà¸•à¹‰à¸­à¸‡à¹€à¸£à¸µà¸¢à¸à¸œà¹ˆà¸²à¸™à¸§à¸´à¸˜à¸µà¸­à¸·à¹ˆà¸™ à¸«à¸£à¸·à¸­à¸žà¸¶à¹ˆà¸‡à¸žà¸² setActive
            if (refs.current.session) {
                refs.current.stream?.getTracks().forEach((t: any) => t.stop());
                refs.current.audioCtx?.close();
                refs.current.session?.close();
                setActive(false);
                setStatus('idle');
            }
        }, 8000); // 8 à¸§à¸´à¸™à¸²à¸—à¸µà¸¥à¸°à¸à¸±à¸™à¸„à¸£à¸±à¸š 5 à¸§à¸´à¸ªà¸±à¹‰à¸™à¹„à¸›à¸™à¸´à¸”à¸™à¸¶à¸‡
    }
    return () => clearTimeout(timeout);
  }, [status, active]);

  // à¸ªà¹ˆà¸‡à¸‚à¹‰à¸­à¸„à¸§à¸²à¸¡à¹à¸—à¸™à¸à¸²à¸£à¸žà¸¹à¸”
  const sendText = useCallback((message: string) => {
    if (!refs.current.session) {
      console.warn('âš ï¸ Session not active');
      return;
    }
    setStatus('thinking');
    refs.current.session.sendClientContent({
      turns: [{ role: 'user', parts: [{ text: message }] }]
    });
    console.log('ðŸ“ Text sent:', message);
  }, []);

  const toggle = useCallback(async () => {
    if (active) {
      refs.current.stream?.getTracks().forEach((t: any) => t.stop());
      refs.current.audioCtx?.close();
      refs.current.session?.close();
      setActive(false);
      setStatus('idle');
      if (speakTimeout.current) clearTimeout(speakTimeout.current);
      return;
    }

    try {
      const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
      const audioCtx = new AudioContext({ sampleRate: SAMPLE_RATE });
      const genAI = new GoogleGenAI({ apiKey: process.env.NEXT_PUBLIC_GEMINI_API_KEY! });

      let nextStartTime = 0;

      const session = await genAI.live.connect({
        model: 'gemini-2.5-flash-native-audio-preview-12-2025',
        config: {
          systemInstruction: { parts: [{ text: instructions.instructions }] },
          responseModalities: [Modality.AUDIO],
          tools: instructions.tools as any,
          speechConfig: { voiceConfig: { prebuiltVoiceConfig: { voiceName: 'Puck' } } }
        },
        callbacks: {
          onmessage: async (msg) => {
            // Handle Tool Calls
            const toolCall = msg.toolCall;
            if (toolCall?.functionCalls) {
              setStatus('thinking');
              for (const fc of toolCall.functionCalls) {
                console.log(`ðŸ”§ Tool called: ${fc.name}`, fc.args);

                if (!fc.name || !fc.id) continue;

                // Special handling for close_session
                if (fc.name === 'close_session') {
                    console.log('ðŸ‘‹ Closing session via tool call');
                    session.sendToolResponse({
                        functionResponses: [{ id: fc.id, name: fc.name, response: { success: true } }]
                    });
                     // Close everything
                    stream.getTracks().forEach((t: any) => t.stop());
                    audioCtx.close();
                    session.close();
                    setActive(false);
                    setStatus('idle');
                    return;
                }

                const handler = toolHandlers[fc.name];
                if (handler) {
                  try {
                    const result = await handler();
                    console.log(`âœ… Tool result:`, result);

                    // à¸ªà¹ˆà¸‡ response à¸à¸¥à¸±à¸šà¹„à¸›à¹ƒà¸«à¹‰ Gemini
                    session.sendToolResponse({
                      functionResponses: [{
                        id: fc.id,
                        name: fc.name,
                        response: result
                      }]
                    });
                  } catch (error) {
                    console.error(`âŒ Tool error:`, error);
                    session.sendToolResponse({
                      functionResponses: [{
                        id: fc.id,
                        name: fc.name,
                        response: { error: String(error) }
                      }]
                    });
                  }
                } else {
                  console.warn(`âš ï¸ Unknown tool: ${fc.name}`);
                }
              }
              return;
            }


            // Handle audio response
            const audioData = msg.serverContent?.modelTurn?.parts?.[0]?.inlineData?.data;
            if (!audioData) return;

            const float32 = base64ToFloat32(audioData);
            const buffer = audioCtx.createBuffer(1, float32.length, SAMPLE_RATE);
            buffer.copyToChannel(float32 as any, 0);

            const source = audioCtx.createBufferSource();
            source.buffer = buffer;
            source.connect(audioCtx.destination);

            const playTime = Math.max(audioCtx.currentTime, nextStartTime);
            source.start(playTime);
            nextStartTime = playTime + buffer.duration;
            
            // Set stats to speaking
            setStatus('speaking');
            
            // Reset to listening after audio finishes
            if (speakTimeout.current) clearTimeout(speakTimeout.current);
            speakTimeout.current = setTimeout(() => {
                setStatus('listening');
            }, (buffer.duration * 1000) + 200); // Add small buffer
          }
        }
      });

      const processor = audioCtx.createScriptProcessor(4096, 1, 1);
      processor.onaudioprocess = (e) => {
        const inputData = e.inputBuffer.getChannelData(0);
        session.sendRealtimeInput({
          media: {
            data: floatToBase64PCM(inputData),
            mimeType: `audio/pcm;rate=${SAMPLE_RATE}`
          }
        });
      };

      audioCtx.createMediaStreamSource(stream).connect(processor);
      processor.connect(audioCtx.destination);

      refs.current = { stream, audioCtx, session };
      setActive(true);
      setStatus('listening');
    } catch (error) {
      console.error('Failed to start Jarvis:', error);
      setActive(false);
      setStatus('idle');
    }
  }, [active]);

  return { active, toggle, sendText, status };
}
