'use client';

import { useRef, useState, useCallback, useEffect } from 'react';
import { GoogleGenAI, Modality } from '@google/genai';
import { base64ToFloat32, floatToBase64PCM } from '@/lib/audioConverter';
import instructions from '@/lib/instructions.json';
import { say_hello } from '@/app/actions/say_hello';

const SAMPLE_RATE = 24000;

// Tool handlers - à¹€à¸žà¸´à¹ˆà¸¡ function à¹ƒà¸«à¸¡à¹ˆà¹„à¸”à¹‰à¸—à¸µà¹ˆà¸™à¸µà¹ˆ
const toolHandlers: Record<string, () => Promise<any>> = {
  say_hello: say_hello,
};

export type JarvisStatus = 'idle' | 'listening' | 'thinking' | 'speaking';

export function useJarvis() {
  const [active, setActive] = useState(false);
  const [status, setStatus] = useState<JarvisStatus>('idle');
  const refs = useRef<any>({});
  const speakTimeout = useRef<NodeJS.Timeout | null>(null);

  // à¸„à¹ˆà¸² timeout (ms) à¸—à¸µà¹ˆà¸ˆà¸°à¸£à¸­à¸«à¸¥à¸±à¸‡à¸ˆà¸²à¸ chunk à¸ªà¸¸à¸”à¸—à¹‰à¸²à¸¢ à¸–à¹‰à¸²à¹„à¸¡à¹ˆà¸¡à¸µ chunk à¹ƒà¸«à¸¡à¹ˆà¸¡à¸² â†’ à¸–à¸·à¸­à¸§à¹ˆà¸²à¸ˆà¸šà¸à¸²à¸£à¸žà¸¹à¸”
  const SPEAKING_TIMEOUT_MS = 4000; // à¸›à¸£à¸±à¸šà¹„à¸”à¹‰ 2200â€“3500 à¸•à¸²à¸¡à¸„à¸§à¸²à¸¡à¹€à¸«à¸¡à¸²à¸°à¸ªà¸¡

  // Auto-close if listening for too long
  useEffect(() => {
    let timeout: NodeJS.Timeout;
    if (status === 'listening' && active) {
      timeout = setTimeout(() => {
        console.log('ðŸ’¤ Auto closing due to inactivity...');
        if (refs.current.session) {
          refs.current.stream?.getTracks().forEach((t: any) => t.stop());
          refs.current.audioCtx?.close();
          refs.current.session?.close();
          setActive(false);
          setStatus('idle');
        }
      }, 15000);
    }
    return () => clearTimeout(timeout);
  }, [status, active]);

  // à¸ªà¹ˆà¸‡à¸‚à¹‰à¸­à¸„à¸§à¸²à¸¡à¹à¸—à¸™à¸à¸²à¸£à¸žà¸¹à¸”
  const sendText = useCallback((message: string) => {
    if (!refs.current.session) {
      console.warn('âš ï¸ Session not active');
      return;
    }
    setStatus('thinking');
    refs.current.session.sendClientContent({
      turns: [{ role: 'user', parts: [{ text: message }] }]
    });
    console.log('ðŸ“ Text sent:', message);
  }, []);

  const toggle = useCallback(async () => {
    if (active) {
      refs.current.stream?.getTracks().forEach((t: any) => t.stop());
      refs.current.audioCtx?.close();
      refs.current.session?.close();
      setActive(false);
      setStatus('idle');
      if (speakTimeout.current) clearTimeout(speakTimeout.current);
      return;
    }

    try {
      const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
      const audioCtx = new AudioContext({ sampleRate: SAMPLE_RATE });
      const genAI = new GoogleGenAI({ apiKey: process.env.NEXT_PUBLIC_GEMINI_API_KEY! });

      let nextStartTime = 0;

      const session = await genAI.live.connect({
        model: 'gemini-2.5-flash-native-audio-preview-12-2025',
        config: {
          systemInstruction: { parts: [{ text: instructions.instructions }] },
          responseModalities: [Modality.AUDIO],
          tools: instructions.tools as any,
          speechConfig: { voiceConfig: { prebuiltVoiceConfig: { voiceName: 'Puck' } } }
        },
        callbacks: {
          onmessage: async (msg) => {
            // Handle Tool Calls
            const toolCall = msg.toolCall;
            if (toolCall?.functionCalls) {
              setStatus('thinking');
              for (const fc of toolCall.functionCalls) {
                console.log(`ðŸ”§ Tool called: ${fc.name}`, fc.args);

                if (!fc.name || !fc.id) continue;

                if (fc.name === 'close_session') {
                  console.log('ðŸ‘‹ Closing session via tool call');
                  session.sendToolResponse({
                    functionResponses: [{ id: fc.id, name: fc.name, response: { success: true } }]
                  });
                  stream.getTracks().forEach((t: any) => t.stop());
                  audioCtx.close();
                  session.close();
                  setActive(false);
                  setStatus('idle');
                  return;
                }

                const handler = toolHandlers[fc.name];
                if (handler) {
                  try {
                    const result = await handler();
                    console.log(`âœ… Tool result:`, result);
                    session.sendToolResponse({
                      functionResponses: [{
                        id: fc.id,
                        name: fc.name,
                        response: result
                      }]
                    });
                  } catch (error) {
                    console.error(`âŒ Tool error:`, error);
                    session.sendToolResponse({
                      functionResponses: [{
                        id: fc.id,
                        name: fc.name,
                        response: { error: String(error) }
                      }]
                    });
                  }
                } else {
                  console.warn(`âš ï¸ Unknown tool: ${fc.name}`);
                }
              }
              return;
            }

            // â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            // Handle audio response (à¸ªà¹ˆà¸§à¸™à¸—à¸µà¹ˆà¹à¸à¹‰à¹„à¸‚à¸«à¸¥à¸±à¸)
            // â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            const audioData = msg.serverContent?.modelTurn?.parts?.[0]?.inlineData?.data;
            if (audioData) {
              const float32 = base64ToFloat32(audioData);
              const buffer = audioCtx.createBuffer(1, float32.length, SAMPLE_RATE);
              buffer.copyToChannel(float32 as any, 0);

              const source = audioCtx.createBufferSource();
              source.buffer = buffer;
              source.connect(audioCtx.destination);

              const playTime = Math.max(audioCtx.currentTime, nextStartTime);
              source.start(playTime);
              nextStartTime = playTime + buffer.duration;

              // à¸•à¸±à¹‰à¸‡ / reset timeout à¸—à¸¸à¸à¸„à¸£à¸±à¹‰à¸‡à¸—à¸µà¹ˆà¸¡à¸µ chunk à¹ƒà¸«à¸¡à¹ˆ
              setStatus('speaking');

              if (speakTimeout.current) {
                clearTimeout(speakTimeout.current);
              }

              speakTimeout.current = setTimeout(() => {
                setStatus('listening');
                console.log('ðŸ—£ï¸ No more audio chunks â†’ back to listening');
              }, SPEAKING_TIMEOUT_MS);
            }
          }
        }
      });

      const processor = audioCtx.createScriptProcessor(4096, 1, 1);
      processor.onaudioprocess = (e) => {
        const inputData = e.inputBuffer.getChannelData(0);
        session.sendRealtimeInput({
          media: {
            data: floatToBase64PCM(inputData),
            mimeType: `audio/pcm;rate=${SAMPLE_RATE}`
          }
        });
      };

      audioCtx.createMediaStreamSource(stream).connect(processor);
      processor.connect(audioCtx.destination);

      refs.current = { stream, audioCtx, session };
      setActive(true);
      setStatus('listening');
    } catch (error) {
      console.error('Failed to start Jarvis:', error);
      setActive(false);
      setStatus('idle');
    }
  }, [active]);

  return { active, toggle, sendText, status };
}